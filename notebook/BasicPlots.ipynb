{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a197bf",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Motivation \n",
    "\n",
    "The motivation for this document is to shade some light on the question of the best hashmap to be used in a particular application. Coming from a strong financial market data experience, I have watched several arguments for using the standard STL, several generic Github repos and commercial implementations. Arguments pro and con within usually a very heated argumentation that nonetheless lack measurable data! \n",
    "\n",
    "As an experienced engineer, I know better that compilers, computer micro-architectures and memory technologies change almost completely every five years, not always for the better. DDR Memory for example has almost tripled latency in the past ten years, which comes to a shock to many.\n",
    "\n",
    "In our practice, clients (often traders and quants) complain looking at the final observable symptom: we are losing trades, we are being picked off by the market, we cannot hit that quote, my backtest is too slow. It is our expertise to drill down and find out what is happening, which is one of the services that keeps us growing every month. In those cases it is often possible to replace components and obtain a quick win that restores profitability, but the reach is obviously limited due to the complexity of dealing with (often undocumented) legacy code.\n",
    "\n",
    "From time to time, we have been blessed with the opportunity to build a system from the ground up, which was the case with ULLTRA at JP Morgan that fuels most of the markets low latency trading (commodities, FX, rates, credit). In these cases, it is imperative to start with componentes tailored for the task, paying particular attention to data containers and related operations. \n",
    "\n",
    "In this context, we wanted to summarize the results we have gathered in the past to support our decisions and share them with a broader audience (you reader).\n",
    "\n",
    "## Scope of Study\n",
    "\n",
    "This document will be limited to a set of 27 publicly-available data containers. Although others exist, like in Intel TBB, either their usage is not fully licensed or their participation in the market does not warrant inspection. \n",
    "\n",
    "We will limit testing to a couple of hardware architectures: Intel Platinum, which is the current top computing hardware at AWS and AMD Ryzen 3, although on a consumer-level device. \n",
    "\n",
    "This study focus on low latency trading as target application. As such, C++ is the only language tested here although containers available for other languages (python dict) will also be analized. \n",
    "\n",
    "## Acknowledgements \n",
    "\n",
    "Running of these tests take typically 24 hours for each batch in the cloud (AWS) and many batches were necessary to create this report. To pay for it, this work has been supported by Vitorian LLC, which I am a founder, and by several contributions from people over the internet to our Paypal.me account: https://paypal.me/henriquebucher \n",
    "\n",
    "I want to thank you all for the generous contributions! \n",
    "\n",
    "This work was partially based on previous works by Nick Welch, Tessil (Thibaut Goetghebuer-Planchon) and Aleksey Cheusov. Thank you for all your hard work!\n",
    "\n",
    "\n",
    "# Containers\n",
    "\n",
    "Data containers have different characteristics. Some are generic one-size-fits-all that can be even STL-compliant like several boost containers. Others are specialty containers that specialize in a type of key like JudyL and JudyHS. \n",
    "\n",
    "Some containers are meant to contain very few entries while others specialize in managing cache under intense amount of data influx. Some containers have a strict commercial license and the majority are open source under a miryad of licenses (GPL, LGPL, MIT, etc).\n",
    "\n",
    "The interfaces are also varied: some provide a modern C++ interface, some a classic C++ interface, some a C-style interface and some a Python interface. We will not use the python interface though. \n",
    "\n",
    "Some containers specialize in pure speed, some focus in raw throughput and others specifically target cache, branch and page impact across the application. \n",
    "\n",
    "We should keep in mind these distinct requirements when analizing the data to avoid a non intelligent binary vision of trying to immediately identify which one is better. If one data container plain suck with no positive tradeoff, I will be the first one to flag for you.\n",
    "\n",
    "The following is a list of the 25 containers we will analize in this work, loosely classified by their usage group.\n",
    "\n",
    "## Standard\n",
    "\n",
    "These are the containers that come for free with your compiler or are usually installed in your distribution due to the large user base. \n",
    "\n",
    "- std_map \n",
    "\n",
    "std::map is contained in the C++ standards since the first SGI standard template library version from 1990. std::map is a sorted associative container that contains key-value pairs with unique keys. It is not a hash map as all others in this report but is here for the banana size comparison. Reference: https://en.cppreference.com/w/cpp/container/map\n",
    "\n",
    "- std_unordered_map\n",
    "\n",
    "std::unordered_map was first introduced with the 2007 TR1 (ISO/IEC RT 19768:2007) that was the preview for C++11 aka \"Modern C++\". It is the standard hash map in the standard template library (STL). Reference: https://en.cppreference.com/w/cpp/container/unordered_map\n",
    "\n",
    "- python3_dict\n",
    "\n",
    "dict is the main hashmap in the python language. It is unordered but it preserves insertion order by using extra information in the header.  \n",
    "Reference: https://github.com/python/cpython/blob/main/Objects/dictobject.c\n",
    "\n",
    "- boost: unordered_map\n",
    "\n",
    "Very similar to STL's std::unordered_map, boost::unordered_map is an unordered associative container that associates unique keys with another value. It is contained inside the boost unordered library. Reference: https://www.boost.org/doc/libs/1_77_0/doc/html/unordered.html\n",
    "\n",
    "- glib: tree, hash_table\n",
    "\n",
    "GLib is a general-purpose, portable utility library, which provides many useful data types, macros, type conversions, string utilities, file utilities, a mainloop abstraction, and so on. It is maintained by the GTK Development Team. https://docs.gtk.org/glib/index.html\n",
    "\n",
    "The GTree struct is an opaque data structure representing a [balanced binary tree][glib-Balanced-Binary-Trees]. https://docs.gtk.org/glib/struct.Tree.html\n",
    "\n",
    "The GHashTable struct is an opaque data structure to represent a [Hash Table][glib-Hash-Tables]. https://docs.gtk.org/glib/struct.HashTable.html\n",
    "\n",
    "## Github/FOSS\n",
    "\n",
    "- carnegie melon: cuckoohash_map\n",
    "\n",
    "libcuckoo provides a high-performance, compact hash table that allows multiple concurrent reader and writer threads. Reference: https://github.com/efficient/libcuckoo\n",
    "\n",
    "- emilib: hash_map\n",
    "\n",
    "emilib \"is a loose collection of C++14 libraries I tend to reuse between different (mostly game-related) projects.  They are meant mostly for me (Emil Ernerfeldt), but if you find them useful, have at it.\"\n",
    "\n",
    "hash_map is a cache-friendly hash table with open addressing, linear probing and power-of-two capacity. Reference: https://github.com/emilk/emilib/blob/master/emilib/hash_map.hpp\n",
    "\n",
    "- google: dense_hash_map, sparse_hash_map\n",
    "\n",
    "**dense_hash_map** and **sparse_hash_map** are Hashed Associative Containers that associate objects of type Key with objects of type Data. They are Pair Associative Containers, meaning that their value type is pair<const Key, Data>. They are also Unique Associative Containers, meaning that no two elements have keys that compare equal using EqualKey.\n",
    "\n",
    "**dense_hash_map** is distinguished from other hash-map implementations by its speed and by the ability to save and restore contents to disk. On the other hand, this hash-map implementation can use significantly more space than other hash-map implementations, and it also has requirements -- for instance, for a distinguished \"empty key\" -- that may not be easy for all applications to satisfy.\n",
    "\n",
    "This class is appropriate for applications that need speedy access to relatively small \"dictionaries\" stored in memory, and for applications that need these dictionaries to be persistent.\n",
    "Reference: http://goog-sparsehash.sourceforge.net/doc/dense_hash_map.html\n",
    "\n",
    "**sparse_hash_map** is distinguished from other hash-map implementations by its stingy use of memory and by the ability to save and restore contents to disk. On the other hand, this hash-map implementation, while still efficient, is slower than other hash-map implementations, and it also has requirements -- for instance, for a distinguished \"deleted key\" -- that may not be easy for all applications to satisfy.\n",
    "\n",
    "This class is appropriate for applications that need to store large \"dictionaries\" in memory, and for applications that need these dictionaries to be persistent.\n",
    "Reference: http://goog-sparsehash.sourceforge.net/doc/sparse_hash_map.html\n",
    "\n",
    "- anonymous: khash\n",
    "\n",
    "khash is a generic hash library in C. It uses macros as templates. khash belongs in a library called klib, which is owned by the \"Attractive Chaos\" github user, which remains unnamed. \n",
    "Reference: https://attractivechaos.wordpress.com/2008/09/02/implementing-generic-hash-library-in-c/\n",
    "\n",
    "- skarupke: bytell_hash_map, flat_hash_map, flat_hash_map_power_of_two\n",
    "\n",
    "These are all described as \"very fast hash tables\". But in the blog descriptions, the author comes up with more details. \n",
    "\n",
    "**flat_hash_map** is a hash table with the following design choices: open addressing, linear probing, robing hood hashing, prime number amount of slots (but I provide an option for using powers of two), with an upper limit on the probe count. \n",
    "\n",
    "**bytell_hash_map** is a chaining hash table. But it’s a chaining hash table in a flat array. Which means it has all the same memory benefits of open addressing hash tables: Few cache misses and no need to do an allocation on every insert. \n",
    "\n",
    "References: https://github.com/skarupke/flat_hash_map\n",
    "https://probablydance.com/2018/05/28/a-new-fast-hash-table-in-response-to-googles-new-fast-hash-table/\n",
    "\n",
    "- sparsepp: sparse_hash_map\n",
    "\n",
    "sparsepp improves on Google's sparsehash and aims to achieve the following objectives: a drop-in alternative for unordered_map and unordered_set; extremely low memory usage (typically about one byte overhead per entry), and most importantly very small memory overhead when resizing; very efficient, typically faster than your compiler's unordered map/set or Boost's; C++11 support. \n",
    "Reference: https://github.com/greg7mdp/sparsepp\n",
    "\n",
    "- Tessil: array_map, hopscotch_map (w/store_hash), robin_map (w/store_hash), sparse_map\n",
    "\n",
    "**array_map** is a cache conscious hash map and hash set for strings. Thanks to its cache friendliness, the structure provides fast lookups while keeping a low memory usage. \n",
    "Reference: https://github.com/Tessil/array-hash\n",
    "\n",
    "**hopscotch_map** is a C++ implementation of a fast hash map and hash set using open-addressing and hopscotch hashing to resolve collisions. It is a cache-friendly data structure offering better performances than std::unordered_map in most cases and is closely similar to google::dense_hash_map while using less memory and providing more functionalities.\n",
    "Reference: https://github.com/Tessil/hopscotch-map\n",
    "\n",
    "**robin_map** is a C++ implementation of a fast hash map and hash set using open-addressing and linear robin hood hashing with backward shift deletion to resolve collisions.\n",
    "Reference: https://github.com/Tessil/robin-map\n",
    "\n",
    "**sparse_map**  is a C++ implementation of a memory efficient hash map and hash set. It uses open-addressing with sparse quadratic probing. The goal of the library is to be the most memory efficient possible, even at low load factor, while keeping reasonable performances. \n",
    "Reference: https://github.com/Tessil/sparse-map\n",
    "\n",
    "## Commercial \n",
    "\n",
    "- judy: HS and L\n",
    "\n",
    "A judyL array is an efficient mapping of integer keys to integer values. It is optimized to avoid CPU cache misses as often as possible. Its memory consumption scales smoothly with number of entries, even when the keys are sparsely distributed. \n",
    "Reference: http://judy.sourceforge.net/doc/JudyL_3x.htm\n",
    "\n",
    "A judyHS array is the equivalent of a judyL array but maps integer keys to strings. \n",
    "Reference: http://judy.sourceforge.net/doc/JudyHS_3x.htm\n",
    "\n",
    "Article: https://preshing.com/20130107/this-hash-table-is-faster-than-a-judy-array/\n",
    "\n",
    "- leveldb \n",
    "\n",
    "LevelDB is a key/value store built by Google. It can support an ordered mapping from string keys to string values. The core storage architecture of LevelDB is a log-structured merge tree (LSM), which is a write-optimized B-tree variant. It is optimized for large sequential writes as opposed to small random writes.\n",
    "Reference: https://github.com/google/leveldb\n",
    "\n",
    "- rocksdb\n",
    "\n",
    "RocksDB is a storage engine with key/value interface, where keys and values are arbitrary byte streams. It was developed at Facebook based on LevelDB and provides backwards-compatible support for LevelDB APIs.\n",
    "Reference: https://github.com/facebook/rocksdb/\n",
    "\n",
    "- Qt: qhash\n",
    "\n",
    "QHash is one of Qt's generic container classes. It stores (key, value) pairs and provides very fast lookup of the value associated with a key.\n",
    "Reference: https://doc.qt.io/qt-5/qhash.html\n",
    "\n",
    "- kyotocabinet: hash, stash\n",
    "\n",
    "Kyoto Cabinet is a library of routines to manage a database. It is a multithreaded key-value embedded database manager where both keys and values are composed of variable-length bytes.  Kyoto Cabinet is a successor and modern implementation of UNIX DBM by AT&T in 1979.\n",
    "Reference: https://dbmx.net/kyotocabinet/\n",
    "\n",
    "# Methodology \n",
    "\n",
    "## Test Hardware\n",
    "\n",
    "Tests were run in four distinct hardware micro architectures: AMD Ryzen 3, AMD Zen3/Milan, ARM Graviton2 and Intel Xeon Platinum. The Ryzen 3 platform is the only consumer based (Threadripper) since we could not find any Milan-based cloud instance that supports PMU drivers in the cloud.  Still, we ran tests on AMD Milan for time comparison only.\n",
    "\n",
    "Further down the road in the writing this report, we found one AWS instance with a Zen2 cpu that supports virtual performance counters: c5a.16xlarge. Upon running the benchmarks, we quickly realize that the results were in line with the current Threadripper values and stopped the test.\n",
    "\n",
    "Google Cloud has the Tau VM instances (T2D) available with Ryzen 3 (Milan) cpus but only on early access, which we applied but did not get confirmation up to the writing of this report. We have seen evidence from reports around that cache management and overall performance on Milan is way ahead of the Intel CPUs. \n",
    "\n",
    "The specs of the platforms we used are:\n",
    "\n",
    "- AMD Ryzen Threadripper 3960X 24-Core Processor (24 cores no HT)\n",
    "\n",
    "- Intel Xeon Platinum 8252C CPU @ 3.80GHz (12 cores no HT) (US$ 0.41/hr AWS:m5zn.6xlarge)\n",
    "\n",
    "- AWS Graviton2 ARM Neoverse-1 (r3p1) (16 cores no HT) (US$ 0.32/hr  AWS:c6a.4xlarge)\n",
    "\n",
    "- AMD EPYC 7V13 (Milan) 64 core processor (US$ 0.96/hr on Azure:HB120-64rs_v3)\n",
    "\n",
    "On AMD and Intel, hyperthreading was disabled on the command line by switching off SMT on the kernel command line. There is no hyperthreading on ARM Graviton2.\n",
    "\n",
    "## Container Sizes\n",
    "\n",
    "Typical containers in production can be used with a couple hundred key/value pairs up to several million entries. Less that this and it's not important for this study and more than that becomes the realm of real databases where clustering, sharding and replication takes precedence over raw algorithm performance.\n",
    "\n",
    "To cover that wide range we ran our tests from 200 to 1.8 million keys, increasing key sizes in steps of 10%, which forms an exponential scale with 97 data points for each test. \n",
    "\n",
    "## Test Run Variations\n",
    "\n",
    "Different test types were created by varying the hardware type, the number of process ran on a box and the compilation arguments to generate the binary, ie each test run had a combination of three of the following characteristics: \n",
    "\n",
    "- **Solo**\n",
    "\n",
    "A single process is spawned on an otherwise empty machine. The binary was compiled with `-march=native`. \n",
    "\n",
    "- **Full**\n",
    "\n",
    "As many processes are spanwed on a machine as the number of non-hyperthreaded cores. The binary was compiled with `-march=native`\n",
    "\n",
    "- **X86** \n",
    "\n",
    "Process spawned using a binary compiled with \"-march=x86-64\", ie with no specialized SIMD instructions. \n",
    "\n",
    "- **Intel**\n",
    "\n",
    "Test was ran on an Intel box (AWS). \n",
    "\n",
    "- **AMD**\n",
    "\n",
    "Test was ran on an AMD Threadripper box (on premises)\n",
    "\n",
    "- **ARM**\n",
    "\n",
    "Test was ran on an Amazon Graviton2 box (AWS)\n",
    "\n",
    "- **Epyc**\n",
    "\n",
    "Test was ran on an AMD Epyc 7x (Zen3) box (Azure)\n",
    "\n",
    "## Operations\n",
    "\n",
    "Operations are divided into three main groups, characterized by their key type: integers, small strings and strings. This comprises the majority of hashmap uses in low latency. \n",
    "\n",
    "### Integers\n",
    "\n",
    "For the integers tests, we use hash maps with int64_t as key and int64_t as value.\n",
    "\n",
    "- **insert_random_shuffle_range** \n",
    "\n",
    "Before the test, we generate a vector with the values \\[0, nb_entries\\) and shuffle this vector. Then for each value k in the vector, we insert the key-value pair (k, 1) in the hash map.\n",
    "\n",
    "- **insert_random_full** \n",
    "\n",
    "Random full inserts: memory footprint and execution time (integers). \n",
    "Before the test, we generate a vector of nb_entries size where each value is randomly taken from an uniform random number generator from all possible positive values an int64_t can hold. Then for each value k in the vector, we insert the key-value pair (k, 1) in the hash map.\n",
    "\n",
    "- **insert_random_full_reserve**\n",
    "\n",
    "Random full inserts with reserve: execution time (integers)\n",
    "Same as the random full inserts test but the reserve method of the hash map is called beforehand to avoid any rehash during the insertion. It provides a fair comparison even if the growth factor of each hash map is different.\n",
    "\n",
    "- **reinsert_random_shuffle_range** \n",
    "\n",
    "Random shuffle inserts of existing keys: execution time (integers)\n",
    "\n",
    "- **delete_random_full**\n",
    "\n",
    "Random full deletes: execution time (integers)\n",
    "Before the test, we insert nb_entries elements in the same way as in the random full insert test. \n",
    "\n",
    "We then delete each key one by one in a different and random order than the one they were inserted.\n",
    "\n",
    "- **iteration_random_full**\n",
    "\n",
    "Random full iteration: execution time (integers)\n",
    "Before the test, we insert nb_entries elements in the same way as in the random full inserts test. \n",
    "\n",
    "We then use the hash map iterators to read all the key-value pairs\n",
    "\n",
    "- **read_random_full**\n",
    "\n",
    "Random full reads: execution time (integers)\n",
    "Before the test, we insert nb_entries elements in the same way as in the random full inserts test. \n",
    "\n",
    "We then read each key-value pair in a different and random order than the one they were inserted.\n",
    "\n",
    "- **read_miss_random_full**\n",
    "\n",
    "Random full reads misses: execution time (integers)\n",
    "Before the test, we insert nb_entries elements in the same way as in the random full inserts test. \n",
    "\n",
    "We then generate another vector of nb_entries random elements different from the inserted elements and \n",
    "we try to search for these unknown elements in the hash map.\n",
    "\n",
    "- **read_random_full_after_delete**\n",
    "\n",
    "Random full deletes: execution time (integers)\n",
    "Before the test, we insert nb_entries elements in the same way as in the random full insert test. \n",
    "\n",
    "We then delete each key one by one in a different and random order than the one they were inserted.\n",
    "\n",
    "- **read_random_shuffle_range**\n",
    "\n",
    "Random shuffle reads: execution time (integers)\n",
    "\n",
    "- **reinsert_random_full** \n",
    "\n",
    "Random full inserts of existing keys: execution time (integers)\n",
    "Before the test, we generate a vector of nb_entries size where each\n",
    "value is randomly taken from an uniform random number generator from\n",
    "all possible positive values an int64_t can hold and insert the\n",
    "key-value pairs (k, 1) in the hash map.  Then we shuffle the vector\n",
    "and insert keys again.</p>\n",
    "\n",
    "### Small strings\n",
    "\n",
    "In this case we use hash maps with std::string as key and int64_t as value.\n",
    "Each string is a random generated string of 10-20 alphanumeric characters (+1 for the null terminator). A generated key may look like \"ju1AOoeWT3LdJxL\".\n",
    "\n",
    "- **insert_small_string**\n",
    "\n",
    "Inserts: memory footprint and execution time (small strings)\n",
    "\n",
    "For each entry in the range \\[0, nb_entries\\), we generate a string as key and insert it with the value 1.\n",
    "\n",
    "- **reinsert_small_string**\n",
    "\n",
    "Inserts of existing keys: execution time (small strings)\n",
    "\n",
    "Before the test for each entry in the range [0, nb_entries), we generate a string as key and insert it with the value 1.  Then, we shuffle the vector of keys and insert them again\n",
    "\n",
    "- **insert_small_string_reserve**\n",
    "\n",
    "Inserts with reserve: memory footprint and execution time (small strings)\n",
    "\n",
    "Same as the inserts test but the reserve method of the hash map is called beforehand to avoid any rehash during the insertion. It provides a fair comparison even if the growth factor of each hash map is different.\n",
    "\n",
    "- **read_small_string**\n",
    "\n",
    "Reads: execution time (small strings). \n",
    "\n",
    "Before the test, we insert nb_entries elements in the hash map as in the inserts test. \n",
    "We then read each key-value pair in a different and random order than the one they were inserted.\n",
    "\n",
    "- **read_miss_small_string**\n",
    "\n",
    "Reads misses: execution time (small strings). \n",
    "\n",
    "Before the test, we insert nb_entries elements in the same way as in the inserts test. \n",
    "We then generate nb_entries strings different from the inserted elements and \n",
    "we try to search for these unknown elements in the hash map.\n",
    "\n",
    "- **delete_small_string**\n",
    "\n",
    "Deletes: execution time (small strings)\n",
    "\n",
    "Before the test, we insert nb_entries elements in the hash map as in the inserts test. \n",
    "We then delete each key one by one in a different and random order than the one they were inserted.\n",
    "\n",
    "- **read_small_string_after_delete**\n",
    "\n",
    "Reads after deleting half: execution time (small strings)\n",
    "\n",
    "Before the test, we insert nb_entries elements in the same way as in the inserts test before \n",
    "deleting half of these values randomly. We then try to read all the original values in a \n",
    "different order which will lead to 50\\% hits and 50\\% misses.\n",
    "\n",
    "### Strings\n",
    "\n",
    "- **read_string_after_delete**\n",
    "\n",
    "Reads after deleting half: execution time (small strings)\n",
    "\n",
    "Before the test, we insert nb_entries elements in the same way as in the inserts test before \n",
    "deleting half of these values randomly. We then try to read all the original values in a \n",
    "different order which will lead to 50% hits and 50% misses.\n",
    "\n",
    "- **insert_string**\n",
    "\n",
    "Inserts: memory footprint and execution time (strings)\n",
    "\n",
    "For each entry in the range [0, nb_entries), we generate a string as key and insert it with the value 1.\n",
    "\n",
    "- **insert_string_reserve**\n",
    "\n",
    "Inserts with reserve: execution time (strings)\n",
    "\n",
    "Same as the inserts test but the reserve method of the hash map is called beforehand to avoid any rehash during the insertion. \n",
    "It provides a fair comparison even if the growth factor of each hash map is different.\n",
    "\n",
    "- **reinsert_string**\n",
    "\n",
    "Inserts of existing keys: execution time (strings)\n",
    "\n",
    "Before the test for each entry in the range [0, nb_entries), we generate a string as key and insert it with the value 1.  Then, we shuffle the vector of keys and insert them again.\n",
    "\n",
    "- **read_string**\n",
    "\n",
    "Reads: execution time (strings)\n",
    "\n",
    "Before the test, we insert nb_entries elements in the hash map as in the inserts test. \n",
    "We then read each key-value pair in a different and random order than the one they were inserted.\n",
    "\n",
    "- **read_miss_string**\n",
    "\n",
    "Reads misses: execution time (strings)\n",
    "\n",
    "Before the test, we insert nb_entries elements in the same way as in the inserts test. \n",
    "We then generate nb_entries strings different from the inserted elements and\n",
    "we try to search for these unknown elements in the hash map.\n",
    "\n",
    "- **delete_string**\n",
    "\n",
    "Deletes: execution time (strings)\n",
    "\n",
    "Before the test, we insert nb_entries elements in the hash map as in the inserts test. \n",
    "We then delete each key one by one in a different and random order than the one they were inserted.\n",
    "\n",
    "- **read_string_after_delete**\n",
    "\n",
    "Reads after deleting half: execution time (strings)\n",
    "\n",
    "Before the test, we insert nb_entries elements in the same way as in the inserts test before deleting half of these values randomly. We then try to read all the original values in a different order which will lead to 50\\% hits and 50\\% misses.\n",
    "\n",
    "## Metrics\n",
    "\n",
    "Some of these metrics as load factor, memory size and total time are obtained from the application internal data itself. All others are private to the kernel and need to be collected using Linux Performance Monitoring Counters (PMC) also known as PMUs. In the cloud we rely on drivers for virtual counters, or vPMU to be available. \n",
    "\n",
    "- **Load Factor**\n",
    "\n",
    "Percentage of used memory with respect of total allocated memory for the container. \n",
    "\n",
    "- **Memory Size**\n",
    "\n",
    "Number of bytes allocated per hash entry during the operation.  \n",
    "\n",
    "- **Total Time**\n",
    "\n",
    "Total executed time per hash entry operation, excluding any kernel time.\n",
    "\n",
    "- **Branch Misses** \n",
    "\n",
    "Total number of branch misses occurred per hash entry operation.\n",
    "\n",
    "- **Cache Misses**\n",
    "\n",
    "Total number of cache misses per hash entry operation.\n",
    "\n",
    "- **Branches**\n",
    "\n",
    "Total number of branches executed per hash entry operation.\n",
    "\n",
    "- **Instructions**\n",
    "\n",
    "Total number of instructions per hash entry operation.\n",
    "\n",
    "- **Cycles**\n",
    "\n",
    "Total number of cycles per hash entry operation.\n",
    "\n",
    "- **Page Faults**\n",
    "\n",
    "Total number of page faults per hash entry operation. \n",
    "\n",
    "- **Nanos per Cycle** \n",
    "\n",
    "Total number of nanoseconds per cycle. Lower is better. This is a calculated metric.\n",
    "\n",
    "- **Nanos per Instruction** \n",
    "\n",
    "Total number of nanoseconds per instruction. Lower is better. This is a calculated metric.\n",
    "\n",
    "- **Page Faults**\n",
    "\n",
    "Total number of page faults occurred during each operation for a single hash entry. \n",
    "\n",
    "- **Page Faults Minor**\n",
    "\n",
    "Total number of minor page faults occurred during each operation for a single hash entry.\n",
    "\n",
    "- **Page Faults Major**\n",
    "\n",
    "Total number of major page faults occurred during each operation for a single hash entry. \n",
    "\n",
    "- **Stalled Cycles Frontend**\n",
    "\n",
    "Number of cycles wasted waiting for instructions to be fed into the pipeline. This can mean misses in the instruction cache, or complex instructions that are not already decoded in the micro-op cache, for example. \n",
    "\n",
    "- **Stalled Cycles Backend**\n",
    "\n",
    "Number of cycles wasted waiting for resources like memory or to finish long latency instructions as transcedentals, reciprocals, integer divisions.\n",
    "\n",
    "\n",
    "# Test Results\n",
    "\n",
    "Jupyter notebook boilerplate - please ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e42fb0e2",
   "metadata": {
    "hide_input": true,
    "tags": [
     "hide-cells"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from PlotUtils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbda13",
   "metadata": {},
   "source": [
    "## General performance\n",
    "\n",
    "Before diving into the performance metrics, we want to construct a performance baseline from with to discuss the other variants, operations and metrics. For this case we want to analize the most common cases and metrics. \n",
    "\n",
    "The base test runs are the AMD vs Intel running solo, as they are usually run. Later on we will see how a busy machine forces these processes to share resources (cache, branch BTB and pages) and how each of the data containers react to this crowded environment.  \n",
    "\n",
    "So let's load the basic tests - solo runs on AMD/Intel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0a6e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "intelsolo = load_data_file( 'output-intelsolo-perfcounter')\n",
    "amdsolo = load_data_file( 'output-amdsolo-perfcounter' )\n",
    "tests = {\"IntelSolo\":intelsolo, \"AMDSolo\":amdsolo}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ddadd1",
   "metadata": {},
   "source": [
    "We will need to create a list with all containers, operations and metrics, as well as separate between string-only and integer-only containers. This is since new operations, containers and metrics get added every day. This way this text can be regenerated dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1be9639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containers:\n",
      "\ttsl_robin_pg_map,ska_flat_hash_map_power_of_two,google_dense_hash_map_mlf_0_9\n",
      "\temilib_hash_map,python3_dict,ska_bytell_hash_map,tsl_robin_map_store_hash,khash\n",
      "\tjudyHS,tsl_robin_map_mlf_0_9,glib_tree,judyL,boost_unordered_map,leveldb\n",
      "\ttsl_robin_map,tsl_hopscotch_map_mlf_0_5,google_sparse_hash_map,tsl_array_map\n",
      "\tcuckoohash_map,tsl_sparse_map,std_unordered_map,ska_flat_hash_map\n",
      "\ttsl_hopscotch_map_store_hash,kyotocabinet_hash,glib_hash_table\n",
      "\tkyotocabinet_stash,spp_sparse_hash_map,google_dense_hash_map\n",
      "\ttsl_array_map_mlf_1_0,tsl_hopscotch_map,qt_qhash,rocksdb,std_map\n",
      "Operations:\n",
      "\tread_string,read_miss_string,read_string_after_delete\n",
      "\tread_small_string_after_delete,read_random_shuffle_range,delete_string\n",
      "\treinsert_random_shuffle_range,iteration_random_full,insert_small_string\n",
      "\tinsert_random_shuffle_range,read_random_full_after_delete,delete_random_full\n",
      "\tdelete_small_string,read_small_string,read_miss_random_full,insert_string\n",
      "\treinsert_string,insert_string_reserve,reinsert_random_full,read_random_full\n",
      "\tread_miss_small_string,insert_random_full,reinsert_small_string\n",
      "\tinsert_random_full_reserve,insert_small_string_reserve\n",
      "Metrics:\n",
      "\tcycles,timesecs,pagefaults-min,cachemisses,memsize,stallfront,stallback\n",
      "\tbranchmisses,loadfactor,nanospercycle,pagefaults-maj,nanosperinst,pagefaults\n",
      "\tbranches,branchmisspct,instpercycle,instructions\n"
     ]
    }
   ],
   "source": [
    "all_containers = set( extract_container_names(tests) )\n",
    "all_operations = set( extract_operation_names(tests) )\n",
    "all_metrics = set( extract_metric_names(tests) )\n",
    "printset(\"Containers\",all_containers )\n",
    "printset(\"Operations\",all_operations)\n",
    "printset(\"Metrics\",all_metrics) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e753217",
   "metadata": {},
   "source": [
    "Some containers are explicitly dedicated to integers (judyL) and some only work with strings (leveldb, rocksdb, judyHS, tsl_array_map) so a subdivision is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b373dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String-only containers:\n",
      "\tleveldb,tsl_array_map,judyHS,tsl_array_map_mlf_1_0,rocksdb\n",
      "Integer-only containers:\n",
      "\tjudyL\n",
      "Common containers::\n",
      "\ttsl_robin_pg_map,ska_flat_hash_map_power_of_two,google_dense_hash_map_mlf_0_9\n",
      "\tpython3_dict,emilib_hash_map,ska_bytell_hash_map,tsl_robin_map_store_hash,khash\n",
      "\ttsl_robin_map_mlf_0_9,glib_tree,boost_unordered_map,tsl_robin_map\n",
      "\ttsl_hopscotch_map_mlf_0_5,google_sparse_hash_map,cuckoohash_map,tsl_sparse_map\n",
      "\tstd_unordered_map,tsl_hopscotch_map_store_hash,ska_flat_hash_map\n",
      "\tkyotocabinet_hash,glib_hash_table,kyotocabinet_stash,spp_sparse_hash_map\n",
      "\tgoogle_dense_hash_map,tsl_hopscotch_map,qt_qhash,std_map\n"
     ]
    }
   ],
   "source": [
    "string_containers = set( extract_container_names( tests, operations=('read_string',) ) )\n",
    "integer_containers = set( extract_container_names( tests, operations=('read_random_full',) ) )\n",
    "common_containers = integer_containers.intersection( string_containers )\n",
    "string_only_containers = string_containers - integer_containers\n",
    "integer_only_containers = integer_containers - string_containers\n",
    "printset(\"String-only containers\",string_only_containers)\n",
    "printset(\"Integer-only containers\",integer_only_containers)\n",
    "printset(\"Common containers:\", common_containers )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e71dc",
   "metadata": {},
   "source": [
    "For clarity, we use the same x and y limits for all graphs and that becomes an issue when there are containers with disparate performance numbers. As we mix hash maps and binary trees, that's often the case. Therefore to make the graphs legible we have to split the containers in groups. \n",
    "Let's arbitrarily define a metric to separate slow and fast containers as ones that display a latency lower or higher than a given number for a read within a map with half million keys in the hash in the Intel solo run. First, let's grab the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2292660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_values( test, minkeys, operation, metric ):\n",
    "    values = []\n",
    "    for (op,ct),aggvalues in test.items():\n",
    "        numkeys = sorted( [ nkeys for nkeys in aggvalues.keys() if nkeys>=minkeys ] )\n",
    "        latency = int( aggvalues[ numkeys[0] ][metric] )\n",
    "        if op==operation:\n",
    "            values.append( (latency,ct) ) \n",
    "    return values\n",
    "integer_values = pick_values( intelsolo, 500000, 'read_random_full', 'timesecs' )\n",
    "string_values  = pick_values( intelsolo, 500000, 'read_string', 'timesecs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9abe3",
   "metadata": {},
   "source": [
    "Let's now use machine learning magic to classify our values and avoid hardcoding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c94f55a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_161103/3696123279.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# let's classify integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# kmeans requires a 2d array so we fake it. log() makes for better gap detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# let's classify integers\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import math\n",
    "# kmeans requires a 2d array so we fake it. log() makes for better gap detection\n",
    "# we should really use Jenks natural breaks here but this is easier\n",
    "X = np.array( [ (math.log(lat),0) for (lat,ct) in integer_values ] )\n",
    "numclusters = 4\n",
    "kmeans = KMeans(n_clusters=numclusters, random_state=0).fit( X )\n",
    "containers = [ [ integer_values[idx] for idx in np.where( kmeans.labels_==knum )[0] ] \n",
    "              for knum in range(numclusters)  ]\n",
    "integer_values = sorted( containers, key= lambda x: x[0][0])\n",
    "printset( \"Fastest integer containers\", [ \"%s:%d\" % (ct,lat) \n",
    "                                      for lat,ct in sorted(integer_values[0]) ] )\n",
    "printset( \"Fast    integer containers\", [ \"%s:%d\" % (ct,lat) \n",
    "                                      for lat,ct in sorted(integer_values[1]) ] )\n",
    "printset( \"Slow    integer containers\", [ \"%s:%d\" % (ct,lat) \n",
    "                                      for lat,ct in sorted(integer_values[2]) ] )\n",
    "printset( \"Slowest integer containers\", [ \"%s:%d\" % (ct,lat) \n",
    "                                      for lat,ct in sorted(integer_values[3]) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f798fb",
   "metadata": {},
   "source": [
    "The fastest containers were all the Tessil's Robin map, Skarupke's hash (power of two) and khash, all between 12 and 15 nanoseconds. \n",
    "\n",
    "As expected, glib, kyoto (both), python and std::map were the slowest containers, all above the 200 nanoseconds mark. \n",
    "\n",
    "Emilib's hash, Skarupke's flat hashmap, Tessil's Robin+store, Tessil's hopscotch, Google's SPP, Google dense hash map and judyL featured between 17 and 27 nanoseconds. \n",
    "\n",
    "glib's hash table, Qt QHash, std and boost unordered map and Google's sparse hash clocked a decent middle from 33 to 66 nanoseconds. \n",
    "\n",
    "The slowest of the fast group were, non surprisingly, Qt QHash (45ns), cuckoo hash (46ns), boost::unordered_map and std::unordered_map at around 50 nanoseconds. judyL and google::sparse_hash_map were a surprise also at 49 and 66 nanoseconds. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for strings, same drill\n",
    "X = np.array( [ (math.log(lat),0) for (lat,ct) in string_values ] )\n",
    "numclusters = 4\n",
    "kmeans = KMeans(n_clusters=numclusters, random_state=0).fit( X )\n",
    "containers = [ [ string_values[idx] for idx in np.where( kmeans.labels_==knum )[0] ] \n",
    "              for knum in range(numclusters)  ]\n",
    "string_values = sorted( containers, key= lambda x: x[0][0])\n",
    "printset( \"Fastest string containers\", [ \"%s:%d\" % (ct,lat) \n",
    "                                      for lat,ct in sorted(string_values[0]) ] )\n",
    "printset( \"Fast    string containers\", [ \"%s:%d\" % (ct,lat) \n",
    "                                      for lat,ct in sorted(string_values[1]) ] )\n",
    "printset( \"Slow    string containers\", [ \"%s:%d\" % (ct,lat) \n",
    "                                      for lat,ct in sorted(string_values[2]) ] )\n",
    "printset( \"Slowest string containers\", [ \"%s:%d\" % (ct,lat) \n",
    "                                      for lat,ct in sorted(string_values[3]) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96af270",
   "metadata": {},
   "source": [
    "The fastest string containers are actually the majority, pointing to the fact that the handling of the strings themselves dwarfs the actual hash map operations. \n",
    "\n",
    "Glib hash map, which had a median performance with integers, comes on top with strings. \n",
    "\n",
    "Tessil's array map (string only) and his other containers take the cake in the fastest group, joined by Skarupke's flat hash map, emilib's hash map, Google's dense hash map and Spp's sparse hash map.\n",
    "\n",
    "In the faster but not fastest group comes Qt, Google sparse hash, Carnegie Melon's cuckoo hash map, boost and std unordered maps, kyoto cabinet stash, judyHS and Python dictionary.\n",
    "\n",
    "The laggards are kyoto cabinet hash, glib tree map, std::map, rocksdb and levelsdb. \n",
    "\n",
    "We can extend the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de08def",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "other_operations = set(all_operations) - set(('read_string','read_random_full') )\n",
    "best_of = {}\n",
    "#print( \"Calculating fastest containers for:\")\n",
    "integer_vectors = {} \n",
    "string_vectors = {}\n",
    "significant_points = (200000,500000,)\n",
    "for operation in ('read_random_full','insert_random_full','delete_random_full'):\n",
    "    for numpoints in significant_points:\n",
    "        values = pick_values( intelsolo, numpoints, operation, 'timesecs' )\n",
    "        for latency,container in values:\n",
    "            if container in integer_containers and container not in integer_only_containers:\n",
    "                integer_vectors.setdefault(container,[]).append((operation,numpoints,latency))\n",
    "            if container in string_containers:\n",
    "                string_vectors.setdefault( container,[]).append((operation,numpoints,latency))\n",
    "#print( [ (ct,len(v)) for ct,v in integer_vectors.items() ] )                \n",
    "# Cluster the data\n",
    "containers,values = zip( *integer_vectors.items() )\n",
    "#print(containers)\n",
    "#print(values)\n",
    "ops,points,latencies = [ [ [ v[n] for v in vc ] for vc in values ] for n in range(3) ] \n",
    "X = np.log( np.array( latencies ) )\n",
    "numclusters = 4\n",
    "kmeans = KMeans(n_clusters=numclusters, random_state=0).fit( X )\n",
    "clustered_containers = [ [ (idx,containers[idx],values[idx]) for idx in np.where( kmeans.labels_==knum )[0] ] \n",
    "              for knum in range(numclusters)  ]\n",
    "#containers = sorted( containers, key= lambda x: x[0][0])\n",
    "for k,containerset in enumerate(clustered_containers): \n",
    "    cg = kmeans.cluster_centers_[k]\n",
    "    #print( k, cg )\n",
    "    cvalues = containerset[0][2]\n",
    "    #print( cvalues )\n",
    "    #for lat,(op,size,v) in zip(cg,cvalues):\n",
    "    #    print( lat, op, size )\n",
    "    containers = [ container for idx,container,value in containerset ]\n",
    "    \n",
    "    for idx,container,value in containerset:\n",
    "        print( \"\\t\"+ container )\n",
    "    plot_containers( tests, ('timesecs',), containers, ('read_random_full',), \n",
    "                same_y=True )\n",
    "        \n",
    "        #print( clustered_containers[k])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_names = {\n",
    "    'random_shuffle_range': [\n",
    "        'insert_random_shuffle_range', 'reinsert_random_shuffle_range',\n",
    "        'read_random_shuffle_range'\n",
    "    ],\n",
    "    'random_full': [\n",
    "        'insert_random_full', 'reinsert_random_full',\n",
    "        'insert_random_full_reserve',\n",
    "        'read_random_full', 'read_miss_random_full',\n",
    "        'delete_random_full', 'read_random_full_after_delete',\n",
    "        'iteration_random_full'\n",
    "    ],\n",
    "    'small_string': [\n",
    "        'insert_small_string', 'reinsert_small_string',\n",
    "        'insert_small_string_reserve',\n",
    "        'read_small_string', 'read_miss_small_string',\n",
    "        'delete_small_string',\n",
    "        'read_small_string_after_delete'\n",
    "    ],\n",
    "    'string': [\n",
    "        'insert_string', 'reinsert_string',\n",
    "        'insert_string_reserve',\n",
    "        'read_string', 'read_miss_string',\n",
    "        'delete_string',\n",
    "        'read_string_after_delete'\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc173fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"What containers excel:\")\n",
    "for container,cset in best_of.items():\n",
    "    printset( container, sorted(cset), separator=\"  \" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d37f5e",
   "metadata": {},
   "source": [
    "It is apparent that Kyoto Cabinet has performance issues with large databases. This needs to be investigated further as it might be related to the exhaustion of memory in the servers, although the Intel m5zn instance has 96 Gb memory and we are launching only 10 processes with a maximum of 7 million keys, which would require at least 1,000 bytes per key to hit the memory limit. \n",
    "\n",
    "Another key fact that pops up to the eye is that the Intel machine beats the AMD machine on every test running solo, by the timing metric alone, for the slowest integer maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epycsolocomplete  = load_data_file('output-epycsolo-complete')\n",
    "intelsolocomplete = load_data_file('output-intelsolo-complete')\n",
    "armsolocomplete   = load_data_file('output-armsolo-complete')\n",
    "amdsolocomplete   = load_data_file( 'output-amdsolo-complete')\n",
    "epycfullcomplete  = load_data_file('output-epycfull-complete')\n",
    "intelfullcomplete = load_data_file('output-intelfull-complete')\n",
    "armfullcomplete   = load_data_file('output-armfull-complete')\n",
    "amdfullcomplete   = load_data_file( 'output-amdfull-complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "amdsolocomplete = load_data_file( 'output-amdsolo-complete')\n",
    "amdsolocompleteold = load_data_file( 'output-amdsolo-complete.old')\n",
    "amdfullcomplete = load_data_file( 'output-amdfull-complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf48a4b5",
   "metadata": {},
   "source": [
    "It is very important to tune up your machine for the task at hand. Accepting the defaults that comes from your cloud provider can lead to unnecessary, large costs. Fortunately today we have facilities like tuned profiles in all major Linux distributions.\n",
    "In the example below, we ran our usual hashmap benchmark, inserting one key/value pair, and gathered relevant performance metrics for two cases: vanilla (as it comes) and with a tuned performance profile. The vertical axis is the performance metric in the plot subtitle and the horizontal axis is the number of keys in the given hashmap (std::unordered_map in this case). All metrics are normalized per key.\n",
    "Although there was not much difference for branch and cache misses, there is a significant reduction in minor pagefaults, which certainly explains the volatile behavior of the timesecs subplot (nanoseconds per hash key).\n",
    "The number of stalled cycles in both frontend and backend also benefited from our tuning, in particular in the mid and high range.\n",
    "Stay tuned (sorry could not resist) for more insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42097e62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tests = {'With Tuning':amdsolocomplete, 'No Tuning':amdsolocompleteold }\n",
    "metrics = ('branchmisses','cachemisses','pagefaults-min','stallback','stallfront','timesecs')\n",
    "plot_metrics( tests, metrics, ('std_unordered_map',), ('read_random_full',), columns=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752cd78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tests = {'IntelSolo':intelsolocomplete, 'IntelFull': intelfullcomplete, \n",
    "         'EpycSolo':epycsolocomplete, 'EpycFull':epycfullcomplete, \n",
    "         'ARMSolo':armsolocomplete,'ARMFull':armfullcomplete, \n",
    "         'AMDSolo':amdsolocomplete, 'AMDFull':amdfullcomplete }\n",
    "atests = {'IntelSolo':intelsolocomplete,\n",
    "         'EpycSolo':epycsolocomplete, \n",
    "         'AMDFull':amdfullcomplete,\n",
    "         'AMDSolo':amdsolocomplete,   }\n",
    "plot_tests(tests, ('timesecs',), ('std_unordered_map',), None, columns=5 )\n",
    "plot_containers( tests, ('timesecs',), \n",
    "                ('std_unordered_map','std_map','google_dense_hash_map','qt_qhash','python3_dict'), \n",
    "                ('read_random_full','read_random_full_after_delete'), \n",
    "                same_y=False, columns=2 )\n",
    "plot_containers( tests, ('timesecs',), \n",
    "                ('std_unordered_map','std_map','google_dense_hash_map','qt_qhash','python3_dict'), \n",
    "                ('read_small_string',), \n",
    "                same_y=False, columns=2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00450ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "armfull = load_data_file( 'output-armfull-perfcounter' )\n",
    "armsolo = load_data_file( 'output-armsolo-perfcounter2' )\n",
    "intelfull = load_data_file( 'output-intelfull-perfcounter' )\n",
    "intelsolo = load_data_file( 'output-intelsolo-perfcounter')\n",
    "amdfull = load_data_file( 'output-amdfull-perfcounter' )\n",
    "amdsolo = load_data_file( 'output-amdsolo-perfcounter' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c99495",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d638c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = {'ARMFull':armfull, 'ARMSolo': armsolo, 'IntelFull':intelfull, 'IntelSolo':intelsolo, \n",
    "         'AMDFull': amdfull, 'AMDSolo':amdsolo }\n",
    "plot_containers( tests, ('timesecs',), ('std_unordered_map','std_map',), ('read_random_full',), \n",
    "                same_y=False, columns=2 )\n",
    "plot_containers( tests, ('timesecs',), ('std_unordered_map','std_map','leveldb','rocksdb'), ('read_small_string',), \n",
    "                same_y=False, columns=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_containers = ('std_unordered_map', 'boost_unordered_map', 'google_dense_hash_map', \n",
    "                   'google_sparse_hash_map','qt_qhash','ska_bytell_hash_map',\n",
    "                   'ska_flat_hash_map_power_of_two',\n",
    "                  'spp_sparse_hash_map', 'tsl_hopscotch_map', 'tsl_hopscotch_map_store_hash', \n",
    "                  'tsl_robin_map', 'tsl_sparse_map')\n",
    "fast_string_containers = \n",
    "fast_containers = all_containers - slow_string_containers\n",
    "plot_containers( tests, ('timesecs',), fast_containers, ('read_random_full',), columns=5 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_string_containers = fast_containers + ('judyHS',)\n",
    "plot_containers( tests, ('timesecs',), fast_string_containers, ('read_string',), columns=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_containers( tests, ('timesecs',), slow_containers, ('read_random_full',) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902c669",
   "metadata": {},
   "source": [
    "## Effects of optimization flags\n",
    "We ran these tests by compiling two sets of binaries, one with \"-O1\" and another with \"-O3\". The datasets are then loaded and plotted side by side. \n",
    "The first graph shows that timing is not affected for most operations as the results stand on top of each other. One important exception is for lookups (read operations) where performance is significantly affected. The effects are the same for integers, small strings and string equally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3556542d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "amdsoloO1 = load_data_file( 'output-amdsolo-optimo1' )\n",
    "amdsoloO3 = load_data_file( 'output-amdsolo-perfcounter' )\n",
    "testmap = {'Optim -O3':amdsoloO3,'Optim -O1':amdsoloO1}\n",
    "plot_tests_multi( testmap, ('timesecs',), ('std_unordered_map',), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c6e26",
   "metadata": {},
   "source": [
    "The effects on std::map are much less though. There is very little difference between the test runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d698af6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_tests_multi( testmap, ('timesecs',), ('std_map',), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd66b6c",
   "metadata": {},
   "source": [
    "The effect of optimization on cache misses is more pronounced though with an overall 10-20% increase in cache misses due to the extra optimization across the board. \n",
    "There is no noticeable effect on any other metric \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0a4fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_tests_multi( testmap, None, ('std_unordered_map',), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f199a3f",
   "metadata": {},
   "source": [
    "# Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intelfull = load_data_file( 'output-intelfull-perfcounter')\n",
    "intelfullx86 = load_data_file( 'output-intelfull-perfx86' )\n",
    "amdfull = load_data_file( 'output-amdfull-perfcounter' )\n",
    "amdsolo = load_data_file( 'output-amdsolo-perfcounter' )\n",
    "intelsolo = load_data_file( 'output-intelsolo-perfcounter')\n",
    "amdsolox86 = load_data_file( 'output-amdsolo-perfx86' )\n",
    "#intelfullname = 'output-xeon8252c-full'\n",
    "#intelfull = load_data_file( intelfullname )\n",
    "#intelsoloname = 'output-xeon8252c-solo'\n",
    "#intelsolo = load_data_file( intelsoloname )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( set( [ t for t,ct in amdsolo.keys() ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbe527",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mymetrics = ('timesecs','cachemisses','pagefaults-min')\n",
    "mytests = ('insert_string',)\n",
    "myct = ('leveldb','rocksdb','std_map','kyotocabinet_stash','kyotocabinet_hash','glib_tree')\n",
    "testmap = {'AMDSolo':amdsolo} #,'AMDSoloX86':amdsolox86,'IntelFull':intelfull,'IntelFullX86':intelfullx86}\n",
    "results = {'AMDFull':amdfull,'AMDSolo':amdsolo,'AMDSoloX86':amdsolox86,'IntelSolo':intelsolo,'IntelFull':intelfull,'IntelFullX86':intelfullx86}\n",
    "plot_containers_multi( results, mymetrics, myct,mytests)\n",
    "\n",
    "\n",
    "#plot_containers( intelsoloname, intelsolo, mymetrics, myct, mytests )\n",
    "#plot_containers( amdfullname, amdfull, mymetrics, myct, mytests )\n",
    "#plot_containers( intelfullname, intelfull, mymetrics, myct, mytests )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfdd20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mymetrics = ('timesecs','cachemisses')\n",
    "myct = ('std_unordered_map','ska_bytell_hash_map','judyL','tsl_robin_map')\n",
    "mytests = ('insert_random_full','read_random_full',)\n",
    "plot_tests( amdsoloname, amdsolo, mymetrics, myct, mytests )\n",
    "#plot_tests( intelsoloname, intelsolo, mymetrics, myct, mytests )\n",
    "#plot_tests( amdfullname, amdfull, mymetrics, myct, mytests )\n",
    "#plot_tests( intelfullname, intelfull, mymetrics, myct, mytests )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50abfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ('cachemisses','branchmisses')\n",
    "plot_containers( amdfullname, amdfull, ('instructions',), myct, tests )\n",
    "plot_containers( intelfullname, intelfull, ('instructions',), myct, tests )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d821f70",
   "metadata": {},
   "source": [
    "On the Graviton2 box one can see that there is no shared cache. Every core has a cache of  its own.\n",
    "\n",
    "On the AMD Ryzen3 box the L1 and L2 caches are individual but the L3 cache is shared across three other cores. It limits the amount of interference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035c08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Henrique Bucher"
   }
  ],
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_metadata": {
   "affiliation": "Vitorian LLC",
   "author": "Henrique Bucher",
   "bib": "notebook.bib",
   "title": "Hash Map Performance Evaluation"
  },
  "title": "Execution Characterization of Several C++ Hashmap Implementations"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
